# -*- coding: utf-8 -*-
"""Wines_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4dRxg4dL9FYPhDiWPxFg4MVYDas7DNA

## Classification of wines with different algorithms of Machine Learning and choosing the best model.

# ***Target: To Explore different algorithms of ML for multi-class classification.***

## Author: Lomovtsev Pavel IMBO-01-19

## *0.* *Data preprocessing.*
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline

import operator

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, roc_curve, roc_auc_score
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import BernoulliNB, GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.svm import LinearSVC

data = pd.read_csv('winequalityN.csv')

data.head()

data.dtypes

X = data.drop(['quality', 'type'], axis=1)
y = data.quality

X[X.isna()] = 0

print(sum(y.isna()))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)

"""# *1. Decision tree classifier*"""

tree = DecisionTreeClassifier(random_state=42)

search = GridSearchCV(tree , param_grid={'criterion' : ['gini', 'entropy'], 'max_depth' : np.arange(1, 15)})
search.fit(X_train, y_train)
best_model = search.best_estimator_

print(f'Score of Decision Tree Classifier: {best_model.score(X_test, y_test)}\nRecall: {recall_score(y_test, best_model.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_model.predict(X_test), average="macro")}')

plt.hist([y_test,best_model.predict(X_test)], color=['red', 'blue'])
plt.legend(['real', 'predicted'])
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.title('Decision Tree classification')
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *2. Random Forest Classifier*"""

random_forest = RandomForestClassifier(random_state=42)

params_rf = {'n_estimators' : np.arange(10, 20),  'max_depth' : np.arange(5, 15), 'criterion' : ['gini', 'entropy']}

search_rf = GridSearchCV(random_forest, param_grid=params_rf)

search_rf.fit(X_train, y_train)

best_rf = search_rf.best_estimator_

print(f'Score of Random Forest Classifier: {best_rf.score(X_test, y_test)}\nRecall: {recall_score(y_test, best_rf.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_rf.predict(X_test), average="macro")}')

plt.hist([y_test,best_rf.predict(X_test)], color=['green', 'blue'])
plt.legend(['real', 'predicted'])
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.title('Random Forest classification')
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *3. K-nearest neighbours Classifier.*"""

kneighbours = KNeighborsClassifier()
params_knn = {'n_neighbors' : np.arange(1,10), 'weights' : ['distance', 'uniform']}
search_knn = GridSearchCV(kneighbours, param_grid=params_knn)
search_knn.fit(X_train, y_train)

best_knn = search_knn.best_estimator_

print(f'Accuracy of KNN Classifier: {accuracy_score(y_test, best_knn.predict(X_test))}\nRecall score: {recall_score(y_test, best_knn.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_knn.predict(X_test), average="macro")}')

plt.hist([y_test, best_knn.predict(X_test)], color=['cyan', 'pink'])
plt.title('KNN classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *4. Ridge Classifier* 
- This is the L1 - reguzarization model.
"""

ridge = RidgeClassifier(random_state=42)
search_ridge = GridSearchCV(ridge, param_grid={'alpha' : np.arange(0.1, 0.9, 0.1)})
search_ridge.fit(X_train, y_train)

best_ridge = search_ridge.best_estimator_

print(f'Accuracy of Ridge Classifier: {accuracy_score(y_test, best_ridge.predict(X_test))}\nRecall score: {recall_score(y_test, best_ridge.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_ridge.predict(X_test), average="macro")}')

plt.hist([y_test, best_ridge.predict(X_test)], color=['violet', 'brown'])
plt.title('Ridge l-1 classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *5. Neural Network*

"""

mlp = MLPClassifier(random_state=42)
search_mlp = GridSearchCV(mlp, param_grid={'hidden_layer_sizes' : [(100,), (150,), (200, )],
                                           'activation' : ['logistic', 'tahn'],
                                           'solver' : ['adam', 'lbfgs'],
                                           'batch_size' : np.arange(100, 200, 50),
                                           'max_iter' : np.arange(100, 200, 50)},
                                           )
search_mlp.fit(X_train, y_train)

best_mlp = search_mlp.best_estimator_

print(f'Accuracy of KNN Classifier: {accuracy_score(y_test, best_mlp.predict(X_test))}\nRecall score: {recall_score(y_test, best_mlp.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_mlp.predict(X_test), average="macro")}')

plt.hist([y_test, best_mlp.predict(X_test)], color=['black', 'orange'])
plt.title('Neural Network classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *6. Naive Bayes*
--------------------------------------------------------

#*6.1. BernoulliNB*
"""

bernoulli = BernoulliNB()

search_bernoulli = GridSearchCV(bernoulli, param_grid={'alpha' : np.arange(0.1, 1.0, 0.1),
                                                       'binarize' : np.arange(0.0, 1.0, 0.1)})

search_bernoulli.fit(X_train, y_train)

best_bernoulli = search_bernoulli.best_estimator_

print(f'Accuracy of Bernoulli Classifier: {accuracy_score(y_test, best_bernoulli.predict(X_test))}\nRecall score: {recall_score(y_test, best_bernoulli.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_bernoulli.predict(X_test), average="macro")}')

plt.hist([y_test, best_bernoulli.predict(X_test)], color=['yellow', 'lime'])
plt.title('Bernoulli classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *6.2. GaussianNB*"""

gauss = GaussianNB()
gauss.fit(X_train, y_train)

print(f'Accuracy of Bernoulli Classifier: {accuracy_score(y_test, gauss.predict(X_test))}\nRecall score: {recall_score(y_test, gauss.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, gauss.predict(X_test), average="macro")}')

plt.hist([y_test, best_bernoulli.predict(X_test)], color=['red', 'blue'])
plt.title('Gaussian classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *7. Logistic Regression*"""

log_reg = LogisticRegression(multi_class='multinomial', random_state=42)

search_log_reg = GridSearchCV(log_reg, param_grid={'penalty' : ['l1', 'l2'],
                                                   'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
                                                   })

search_log_reg.fit(X_train, y_train)

best_log_reg = search_log_reg.best_estimator_

print(f'Accuracy of Logistic Regression Classifier: {accuracy_score(y_test, best_log_reg.predict(X_test))}\nRecall score: {recall_score(y_test, best_log_reg.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_log_reg.predict(X_test), average="macro")}')

plt.hist([y_test, best_log_reg.predict(X_test)], color=['green', 'violet'])
plt.title('Logistic Regression classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *8. Quadratic Discriminant Analysis*"""

discriminant = QuadraticDiscriminantAnalysis()
search_discriminant = GridSearchCV(discriminant, param_grid={
                                                             'reg_param' : np.arange(0.0, 1.0, 0.1),
                                                             'tol' : np.arange(0.0001, 0.0010, 0.0001)})

search_discriminant.fit(X_train, y_train)

best_discriminant = search_discriminant.best_estimator_

print(f'Accuracy of Quadratic Discriminant Classifier: {accuracy_score(y_test, best_discriminant.predict(X_test))}\nRecall score: {recall_score(y_test, best_discriminant.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_discriminant.predict(X_test), average="macro")}')

plt.hist([y_test, best_discriminant.predict(X_test)], color=['yellow', 'blue'])
plt.title('Quadratic Discriminant classification')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *9. Linear SVC*"""

svc = LinearSVC(random_state=123)
search_svc = GridSearchCV(svc, param_grid={'penalty' : ['l1', 'l2'],
                                           'loss' : ['hinge', 'squared_hinge'],
                                           'multi_class' : ['ovr', 'crammer_singer'],
                                           })

search_svc.fit(X_train, y_train)

best_svc = search_svc.best_estimator_
print(f'Accuracy of SVM Classifier: {accuracy_score(y_test, best_svc.predict(X_test))}\nRecall score: {recall_score(y_test, best_svc.predict(X_test), average="macro")}\nF1-score: {f1_score(y_test, best_svc.predict(X_test), average="macro")}')

plt.hist([y_test, best_svc.predict(X_test)], color=['yellow', 'blue'])
plt.title('SVC classifier')
plt.xlabel('Quality of wine')
plt.ylabel('No of examples')
plt.legend(['real', 'predicted'])
plt.gcf().set_size_inches(16, 8)
plt.show()

"""# *10. Choosing the best model by Accuracy-score.*"""

models_scores = {'Decision Tree Classifier' : best_model.score(X_test,y_test),
                 'Random Forest Classifier' : best_rf.score(X_test, y_test),
                 'K-Nearest Neighbors Classifier' : best_knn.score(X_test, y_test),
                 'Ridge L1-Classifier' : best_ridge.score(X_test, y_test),
                 'Neural Network' : best_mlp.score(X_test, y_test),
                 'Bernoulli Classifier' : best_bernoulli.score(X_test, y_test),
                 'Gaussian Classifier' : gauss.score(X_test, y_test),
                 'Logistic Regression Classifier' : best_log_reg.score(X_test, y_test),
                 'Quadratic Discriminant Analysis Classifier' : best_discriminant.score(X_test, y_test),
                 'Linear Support Vector Method Classification' : best_svc.score(X_test, y_test)}

THE_BEST_MODEL = max(models_scores.items(), key=operator.itemgetter(1))[0]
THE_BEST_SCORE = max(models_scores.values())

print(f'The best Machine Learning Classifier for WINES QUALITIES is {THE_BEST_MODEL} with score {round(THE_BEST_SCORE*100,2)} %')

"""# ***11. Conclusion***
- The best Machine Learning Classifier for WINES QUALITIES is Random Forest Classifier with score 65.36 %
"""